{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7bb83b",
   "metadata": {},
   "source": [
    "### Experiment Runner \n",
    "\n",
    "Jeff Smith\n",
    "\n",
    "Created: 2025-10-24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e705ee",
   "metadata": {},
   "source": [
    "#### Setup and Connect to Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Creates api object and connects to Portal (parameters in .env)\n",
    "#\n",
    "# Imports - Make sure to delete the __pycache__ directory and restart the kernel if you change helper.py.\n",
    "from helper import *\n",
    "from pysimio import pySimio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import threading\n",
    "from pysimio.classes import TimeOptions\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load environment variables\n",
    "# Makes sure that .env is set up for the portal instance that you want. The sample.env file has\n",
    "# more details.\n",
    "load_dotenv(override=True)\n",
    "simio_portal_url = os.getenv(\"SIMIO_PORTAL_URL\")\n",
    "print(f\"Connecting to portal on: {simio_portal_url}\")\n",
    "personal_access_token = os.getenv(\"PERSONAL_ACCESS_TOKEN\")\n",
    "auth_refresh_time = 500  # Time in seconds to refresh the auth token\n",
    "run_status_refresh_time = 10\n",
    "# Ensure token is loaded\n",
    "if not personal_access_token:\n",
    "    raise ValueError(\"Personal access token not found. Make sure it's set in the environment.\")\n",
    "\n",
    "# API Initialization getting bearer token for authorization\n",
    "api = pySimio(simio_portal_url)\n",
    "api.authenticate(personalAccessToken=personal_access_token)\n",
    "\n",
    "# Start token refresh in a background thread\n",
    "threading.Thread(target=refresh_auth_token, args=(api, auth_refresh_time), daemon=True).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854197c",
   "metadata": {},
   "source": [
    "#### Getting Model/Experiment Information and Deleting Target Runs (if they exists) (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Gets the model, experiment, run information from Portal\n",
    "# model_id and experiment_id are identified in this cell\n",
    "#\n",
    "# Project and Run names\n",
    "project_name = \"Optimization Example 03\"\n",
    "run_name = \"API01\"\n",
    "# For sorting the scenarios after the run - These will be model-specific\n",
    "sort_response = \"Throughput\"\n",
    "sort_ascending = False\n",
    "\n",
    "# Get Model ID for project by project name - Assumes that the project has a single model\n",
    "# (will work if the project has multiple models, but this code return the \"first\" model in the project)\n",
    "all_models_json = api.getModels()\n",
    "model_id = find_modelid_by_projectname(all_models_json, project_name)\n",
    "print(f\"The model_id for project '{project_name}' is {model_id}\")\n",
    "all_experiments_json = api.getExperiments()\n",
    "# Returns the \"first\" experiment for the model\n",
    "experiment_id = find_id_by_model_id(all_experiments_json, model_id)\n",
    "print(f\"The first experiment_id for model {model_id} is {experiment_id}\")\n",
    "runs = get_runs_for_experiment(api, experiment_id)\n",
    "if runs:\n",
    "    print(f\"Existing runs for {run_name} ({len(runs)}): {runs}.\")\n",
    "else:\n",
    "    print(f\"Run {run_name} does not exist for experiment {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually setting the model and experiment (useful if you want\n",
    "# an experiment that's not the first one for the model)\n",
    "model_id = 208\n",
    "experiment_id = 17277\n",
    "run_name = \"API01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Delete existing runs for experiment\n",
    "#\n",
    "runs = get_runs_for_experiment(api, experiment_id)\n",
    "if runs:\n",
    "    delete_runs(api, runs)\n",
    "    print(f\"Deleted runs {runs}.\")\n",
    "else:\n",
    "    print(f\"No runs found for experiment {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad3bfd",
   "metadata": {},
   "source": [
    "#### Create the Experiment Run JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ba1d6",
   "metadata": {},
   "source": [
    "##### Create a Single Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# experiment_id and run_name must be set already\n",
    "#\n",
    "\n",
    "# Set the experiment control values\n",
    "num_reps = 16\n",
    "control_values = [\n",
    "    {\"Name\": \"Buff1\", \"Value\": \"3\"}, \n",
    "    {\"Name\": \"Buff2\", \"Value\": \"3\"}, \n",
    "    {\"Name\": \"Buff3\", \"Value\": \"3\"}, \n",
    "    {\"Name\": \"CapA\",  \"Value\": \"2\"},\n",
    "    {\"Name\": \"CapB\",  \"Value\": \"4\"},\n",
    "    {\"Name\": \"CapC\",  \"Value\": \"2\"},\n",
    "    {\"Name\": \"CapD\",  \"Value\": \"1\"}]\n",
    "scenarios = [\n",
    "    {\"Name\": \"Scenario1\", \"ReplicationsRequired\": num_reps, \"ControlValues\": control_values},\n",
    "]\n",
    "experiment_def = { \"ExperimentId\": experiment_id, \"Name\": run_name, \"CreateInfo\": { \"Scenarios\": scenarios} }\n",
    "n = len(experiment_def[\"CreateInfo\"][\"Scenarios\"])\n",
    "print(f\"Experiment {experiment_def[\"Name\"]} has {n} scenarios and {n*num_reps} total replications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93a21e",
   "metadata": {},
   "source": [
    "##### Generate All Combinations of Control Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# experiment_id and run_name must be set before running this cell\n",
    "#\n",
    "# Experiment definition, experiment_def, is created here. \n",
    "#\n",
    "# These are the experiment controls and values to enumerate over\n",
    "# Specific to the example \"Optimization Model 03.spfx\"\n",
    "vars = {\n",
    "    \"Buff1\" : [1, 2, 3],\n",
    "    \"Buff2\" : [1, 2, 3],\n",
    "    \"Buff3\" : [1, 2, 3],\n",
    "    \"CapA\"  : [1, 2, 3],\n",
    "    \"CapB\"  : [2, 3, 4],\n",
    "    \"CapC\"  : [2, 3, 4],\n",
    "    \"CapD\"  : [2, 3, 4]\n",
    "}\n",
    "num_reps = 6\n",
    "experiment_def = create_ff_experiment(experiment_id, run_name, num_reps, vars)\n",
    "n = len(experiment_def[\"CreateInfo\"][\"Scenarios\"])\n",
    "print(f\"Experiment {experiment_def[\"Name\"]} has {n} scenarios and {n*num_reps} total replications.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40644e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the experiment payload\n",
    "print(json.dumps(experiment_def, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4f010",
   "metadata": {},
   "source": [
    "#### Run the Created Experiment and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276535f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment.\n",
    "#\n",
    "# Need these to be defined before executing this cell:\n",
    "#   experiment_def - the experiment definition json\n",
    "#   experiment_id\n",
    "#   run_name\n",
    "#\n",
    "# Set to True if you want this cell to wait for run completion\n",
    "WaitForRun = False\n",
    "\n",
    "run_id = api.startRun(experiment_def)\n",
    "all_runs_json = api.getRuns()\n",
    "# Get the current date and time\n",
    "current_time_local = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "print(f\"Started run {run_name} ({run_id}) with {len(experiment_def[\"CreateInfo\"][\"Scenarios\"])} scenarios at {current_time_local}.\")\n",
    "theRun = get_run(api, run_id, True)\n",
    "if WaitForRun:\n",
    "    sleep_time = 10 # seconds\n",
    "    status = wait_for_run(api, run_id, sleep_time, 2000, False)\n",
    "    print(f\"Final run status after approx. {status[1]*sleep_time/60:.2f} minutes ({status[1]} cycles): {status[0]}\")\n",
    "else:\n",
    "     print(f\"Not waiting for run to finish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the run Status - run_id must be set\n",
    "#\n",
    "theRun = get_run(api, run_id, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set run_id -- Shouldn't need to run this unless you want to \n",
    "# see the results from a different run than was done above.\n",
    "run_id = 603\n",
    "sort_response = \"Throughput\"\n",
    "sort_ascending = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a94700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment response results\n",
    "#\n",
    "rows_to_show = 8\n",
    "scenarios = api.getScenarios(run_id = run_id)\n",
    "# Convert to dataframe\n",
    "df, responses, controls = scenario_results_as_df(scenarios)\n",
    "# remove any scenarios with non-numeric values in the sort column\n",
    "df[sort_response] = pd.to_numeric(df[sort_response], errors='coerce')\n",
    "df_cleaned = df.dropna(subset=[sort_response])\n",
    "if len(df) > len (df_cleaned):\n",
    "    print(f\"Removed {len(df)-len(df_cleaned)} rows with non-numeric values in the sort column.\")\n",
    "print(f\"Dataframe from run_id {run_id} includes {len(df_cleaned)} scenarios.\")\n",
    "# --- Display the Table ---\n",
    "print(f\"--- Top (up to) {rows_to_show} Scenario Response Averages by {sort_response} ---\")\n",
    "df_sorted = df_cleaned.sort_values(by=sort_response, ascending=sort_ascending)\n",
    "print(df_sorted[:rows_to_show].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ad48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show - In case you want to look at more results without rerunning.\n",
    "#\n",
    "rows_to_show = 100\n",
    "print(f\"Dataframe from run_id {run_id} includes {len(df_cleaned)} scenarios.\")\n",
    "# --- Display the Table ---\n",
    "print(f\"--- Top (up to) {rows_to_show} Scenario Response Averages by {sort_response} ---\")\n",
    "df_sorted = df_cleaned.sort_values(by=sort_response, ascending=sort_ascending)\n",
    "print(df_sorted[:rows_to_show].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, experiment_id, run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d812205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(experiment_def, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
